{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, sum as sum_, max as max_, min as min_, avg, count\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de una sesión Spark para procesar los datos\n",
    "def create_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Data Processing with Spark\") \\\n",
    "        .config(\"spark.jars\", \"../driver/postgresql-42.7.3.jar\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista y ordena los archivos CSV en un directorio que no contengan 'validation' en el nombre\n",
    "def get_csv_files(directory):\n",
    "    return sorted([os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.csv') and 'validation' not in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga un archivo CSV, actualiza las estadísticas y lo guarda en una base de datos PostgreSQL\n",
    "def load_data_and_update_stats(file_name, spark, batch_uuid):\n",
    "    try:\n",
    "        data = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "        data = data.withColumn(\"source_file\", lit(os.path.basename(file_name)))\n",
    "        data = data.withColumn(\"batch_uuid\", lit(batch_uuid))\n",
    "        file_size = os.path.getsize(file_name)\n",
    "        date_process = os.path.getsize(file_name)\n",
    "        data = data.withColumn(\"file_size\", lit(file_size))\n",
    "        data.write.format(\"jdbc\").options(\n",
    "            url='jdbc:postgresql://localhost:5434/pipeline',\n",
    "            driver='org.postgresql.Driver',\n",
    "            dbtable='transaction_data_spark',\n",
    "            user='postgres',\n",
    "            password='developer').mode('append').save()\n",
    "        print(f\"Loaded {file_name} with file size {file_size} bytes and batch UUID {batch_uuid}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load data from {file_name}. Error: {str(e)}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega estadísticas de precio desde los datos y recupera los resultados\n",
    "def aggregate_stats(data):\n",
    "    stats = data.agg(\n",
    "        sum_(\"price\").alias(\"sum_price\"),\n",
    "        avg(\"price\").alias(\"avg_price\"),\n",
    "        min_(\"price\").alias(\"min_price\"),\n",
    "        max_(\"price\").alias(\"max_price\"),\n",
    "        count(lit(1)).alias(\"count\")\n",
    "    ).collect()[0]\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesa todos los archivos en un directorio y valida contra un archivo de validación específico\n",
    "def process_files_and_validate(directory, validation_file):\n",
    "    spark = create_spark_session()\n",
    "    batch_uuid = str(uuid.uuid4())\n",
    "    csv_files = get_csv_files(directory)\n",
    "    all_data = None\n",
    "\n",
    "    for file in csv_files:\n",
    "        data = load_data_and_update_stats(file, spark, batch_uuid)\n",
    "        if all_data is None:\n",
    "            all_data = data\n",
    "        else:\n",
    "            all_data = all_data.union(data)\n",
    "\n",
    "    in_memory_stats = aggregate_stats(all_data)\n",
    "    print(\"Current in-memory stats:\", in_memory_stats.asDict())\n",
    "\n",
    "    print(\"\\nProcessing validation file...\")\n",
    "    validation_data = load_data_and_update_stats(validation_file, spark, batch_uuid)\n",
    "    validation_stats = aggregate_stats(validation_data)\n",
    "    print(\"Validation file stats:\", validation_stats.asDict())\n",
    "\n",
    "    final_data = all_data.union(validation_data)\n",
    "    final_db_stats = aggregate_stats(final_data)\n",
    "    print(\"Final database stats after loading validation.csv:\", final_db_stats.asDict())\n",
    "\n",
    "    return in_memory_stats, validation_stats, final_db_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio y archivo de validación\n",
    "directory = '../input'\n",
    "validation_file = '../input/validation.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 21:00:57 WARN Utils: Your hostname, mugen resolves to a loopback address: 127.0.1.1; using 192.168.68.113 instead (on interface wlp2s0)\n",
      "24/05/12 21:00:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/12 21:00:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ../input/2012-1.csv with file size 374 bytes and batch UUID 36b7c0bd-0be3-4a86-aaf8-96df1d004bda\n",
      "Loaded ../input/2012-2.csv with file size 482 bytes and batch UUID 36b7c0bd-0be3-4a86-aaf8-96df1d004bda\n",
      "Loaded ../input/2012-3.csv with file size 513 bytes and batch UUID 36b7c0bd-0be3-4a86-aaf8-96df1d004bda\n",
      "Loaded ../input/2012-4.csv with file size 495 bytes and batch UUID 36b7c0bd-0be3-4a86-aaf8-96df1d004bda\n",
      "Loaded ../input/2012-5.csv with file size 513 bytes and batch UUID 36b7c0bd-0be3-4a86-aaf8-96df1d004bda\n",
      "Current in-memory stats: {'sum_price': 8046, 'avg_price': 57.884892086330936, 'min_price': 10, 'max_price': 100, 'count': 143}\n",
      "\n",
      "Processing validation file...\n",
      "Loaded ../input/validation.csv with file size 145 bytes and batch UUID 36b7c0bd-0be3-4a86-aaf8-96df1d004bda\n",
      "Validation file stats: {'sum_price': 334, 'avg_price': 41.75, 'min_price': 11, 'max_price': 92, 'count': 8}\n",
      "Final database stats after loading validation.csv: {'sum_price': 8380, 'avg_price': 57.006802721088434, 'min_price': 10, 'max_price': 100, 'count': 151}\n"
     ]
    }
   ],
   "source": [
    "# Ejecución del pipeline\n",
    "final_stats, validation_stats, final_db_stats = process_files_and_validate(directory, validation_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "technical-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
